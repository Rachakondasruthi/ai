---
category: news
title: "Microsoft Open-Sources ONNX Acceleration for BERT AI Model"
excerpt: "According to Ning, With ONNX Runtime, AI developers can now easily productionize large transformer models with high performance across both CPU and GPU hardware, using the same technology Microsoft uses to serve their customers. Taking the lessons learned from re-implementing BERT, the Bing and Azure devs updated the ONNX Runtime code to ..."
publishedDateTime: 2020-01-28T14:04:00Z
webUrl: "https://www.infoq.com/news/2020/01/microsoft-bert-acceleration/"
type: article
quality: 58
heat: -1
published: false

provider:
  name: InfoQ
  domain: infoq.com

topics:
  - AI
  - Microsoft AI
  - AI Hardware
  - Natural Language Processing

images:
  - url: "https://res.infoq.com/news/2020/01/microsoft-bert-acceleration/en/headerimage/microsoft-bert-acceleration-1580052141385.jpg"
    width: 2121
    height: 1414
    title: "Microsoft Open-Sources ONNX Acceleration for BERT AI Model"

related:
  - title: "Google open-sources LaserTagger, an AI model that speeds up text generation"
    excerpt: "Sequence-to-sequence AI models, which were introduced by Google in 2014, aim to map fixed-length input (usually text) with a fixed-length output where the length of the input and output might differ. Theyâ€™re used in text-generating tasks including summarization, grammatical error correction, and sentence fusion, and recent architectural ..."
    publishedDateTime: 2020-01-31T20:06:00Z
    webUrl: "https://venturebeat.com/2020/01/31/google-open-sources-lasertagger-an-ai-model-that-speeds-up-text-generation/"
    ampWebUrl: "https://venturebeat.com/2020/01/31/google-open-sources-lasertagger-an-ai-model-that-speeds-up-text-generation/amp/"
    cdnAmpWebUrl: "https://venturebeat-com.cdn.ampproject.org/c/s/venturebeat.com/2020/01/31/google-open-sources-lasertagger-an-ai-model-that-speeds-up-text-generation/amp/"
    type: article
    provider:
      name: VentureBeat
      domain: venturebeat.com
    quality: 85
    images:
      - url: "https://venturebeat.com/wp-content/uploads/2019/10/google-ai-logo.jpg?fit=1200%2C600&strip=all"
        width: 1200
        height: 600
  - title: "Google Open-Sources Reformer Efficient Deep-Learning Model"
    excerpt: "Researchers from Google AI recently open-sourced the Reformer, a more efficient version of the Transformer deep-learning model. Using a hashing trick for attention calculation and reversible residual layers, the Reformer can handle text sequences up to 1 million words while consuming only 16GB of memory on a single GPU accelerator. With such a ..."
    publishedDateTime: 2020-02-04T14:02:00Z
    webUrl: "https://www.infoq.com/news/2020/02/google-reformer-deep-learning/"
    type: article
    provider:
      name: InfoQ
      domain: infoq.com
    quality: 4

secured: "LQddvVf1UAAAoT8RCBiX06Xm1fGqqRZSEgQ66tjzB6R6WXTl6bClh4dCZK/3xsftRRyXBQ0S7gBGSu8Q9p1w0sRk17e9PhjErGare6VkbbwfWx4yKtFewqVg9ezoI/u9BL8zPKdSThldH2UkzL2vx8w2KgiEMro9K23LYK7WbCYmFezypjdeYIiaOESss0U8QqMxGsJdfRAcLiMgr6TokG6glsHf76K/Zc7+/WSbZfBwAeMH7zXg4poG61sR3NoLEK3UwOjkpYX5tWRlO+DOsAIDg917KafN5VfccJLFNscP5cEIuR0zsuc84qx9SpWLyYrl4ltCgU5dlpTdM8Aj6guiEONRwFpvIovi9TjOgMB7vl19tpZszWD2XMC6cxnS94srmbxMAOfrwUcsnhB4tXNvkmyss22adK9t+h7drR2p6NV5dqRt1rQW+zEKKX+ac4a6kxHcck6jNVkNXz5t5ABiRON+g0JwQafBu8mpDyM=;7lWzondqPAhSdGmP2/8phw=="
---

